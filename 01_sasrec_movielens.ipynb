{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30baedc6-4dea-420e-8b09-b5c28e0bdecb",
   "metadata": {},
   "source": [
    "# SASRec on MovieLens\n",
    "This notebook demonstrates the use of the sequenial recommendation algorithm, **SASRec**, to predict the next movie for a particular user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4deff34-3e09-41da-8761-f384fa5999c4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6f8950d-ea14-416c-abdf-9692b2691da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85e6927-f145-427d-9751-c1770b82b8bf",
   "metadata": {},
   "source": [
    "## Load and prepare data\n",
    "***NOTE***  \n",
    "It is assumed that that MovieLens-1M dataset has already been downloaded and placed next to this notebook in a folder named `ml-1m`.\n",
    "\n",
    "This is what'll happen below:\n",
    "- After loading the data, sort each user's sequence of movie ratings chronologically.\n",
    "- If a user's sequence is less than 3 movie ratings long, use all movie ratings for training. Otherwise, use all but the last two ratings for training, second-to-last rating for validation and the last rating for testing.\n",
    "- After that, we'll sample \"negative\" examples to use for training alongside the actual movies that were selected/rated by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c485237-df34-4713-9e47-6d7cd9f9b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# 1. Preprocess MovieLens 1M\n",
    "# ====================================================\n",
    "ratings = pd.read_csv(\n",
    "    \"ml-1m/ratings.dat\",\n",
    "    sep=\"::\",\n",
    "    engine=\"python\",\n",
    "    names=[\"user\", \"item\", \"rating\", \"timestamp\"]\n",
    ")\n",
    "\n",
    "# Keep ratings >= 4\n",
    "ratings = ratings[ratings[\"rating\"] >= 4]\n",
    "\n",
    "# Map to consecutive IDs\n",
    "user2id = {u: i+1 for i, u in enumerate(ratings[\"user\"].unique())}\n",
    "item2id = {m: i+1 for i, m in enumerate(ratings[\"item\"].unique())}\n",
    "ratings[\"user\"] = ratings[\"user\"].map(user2id)\n",
    "ratings[\"item\"] = ratings[\"item\"].map(item2id)\n",
    "\n",
    "n_users = len(user2id)\n",
    "n_items = len(item2id)\n",
    "\n",
    "# Build user sequences\n",
    "user_sequences = defaultdict(list)\n",
    "for row in ratings.itertuples(index=False):\n",
    "    user_sequences[row.user].append((row.item, row.timestamp))\n",
    "\n",
    "# Sort by time\n",
    "for u in user_sequences:\n",
    "    user_sequences[u] = [x[0] for x in sorted(user_sequences[u], key=lambda x: x[1])]\n",
    "\n",
    "# Leave-one-out split\n",
    "train_seqs, valid_seqs, test_seqs = {}, {}, {}\n",
    "for u, items in user_sequences.items():\n",
    "    if len(items) < 3:\n",
    "        train_seqs[u] = items\n",
    "        valid_seqs[u], test_seqs[u] = [], []\n",
    "    else:\n",
    "        train_seqs[u] = items[:-2]\n",
    "        valid_seqs[u] = [items[-2]]\n",
    "        test_seqs[u] = [items[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "776ca406-5be1-424b-864a-3e7c9e3f1b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# 2. Dataset + Negative Sampling\n",
    "# ====================================================\n",
    "MAX_SEQ_LEN = 50  # max sequence length\n",
    "\n",
    "class SASRecDataset(Dataset):\n",
    "    def __init__(self, user_train, n_items, num_negatives=1):\n",
    "        self.user_train = user_train\n",
    "        self.users = list(user_train.keys())\n",
    "        self.n_items = n_items\n",
    "        self.num_negatives = num_negatives\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        user = self.users[idx]\n",
    "        seq = self.user_train[user]\n",
    "\n",
    "        if len(seq) < 2:\n",
    "            return torch.zeros(MAX_SEQ_LEN, dtype=torch.long), torch.tensor(0, dtype=torch.long), torch.tensor(0, dtype=torch.long)\n",
    "\n",
    "        cut = random.randint(1, len(seq) - 1)\n",
    "        prefix, target = seq[:cut], seq[cut]\n",
    "\n",
    "        # Pad prefix\n",
    "        seq_padded = [0] * (MAX_SEQ_LEN - len(prefix)) + prefix[-MAX_SEQ_LEN:]\n",
    "        seq_tensor = torch.tensor(seq_padded, dtype=torch.long)\n",
    "\n",
    "        # Negative sample\n",
    "        neg = random.randint(1, self.n_items)\n",
    "        while neg in seq:\n",
    "            neg = random.randint(1, self.n_items)\n",
    "\n",
    "        return seq_tensor, torch.tensor(target, dtype=torch.long), torch.tensor(neg, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a40e5f7-c362-40bd-b784-23554bf4415e",
   "metadata": {},
   "source": [
    "## SASRec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fdbd2a4-3c0a-4cb5-8847-bf12a33c14cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# 3. SASRec Model\n",
    "# ====================================================\n",
    "class SASRec(nn.Module):\n",
    "    def __init__(self, n_items, hidden_dim=64, max_len=50, n_heads=2, n_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.item_emb = nn.Embedding(n_items+1, hidden_dim, padding_idx=0)\n",
    "        self.pos_emb = nn.Embedding(max_len, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=hidden_dim*4,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def forward(self, seq):\n",
    "        device = seq.device\n",
    "        positions = torch.arange(self.max_len, device=device).unsqueeze(0)\n",
    "        x = self.item_emb(seq) + self.pos_emb(positions)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        mask = (seq == 0)\n",
    "        x = self.encoder(x, src_key_padding_mask=mask)\n",
    "\n",
    "        out = x[:, -1, :]  # last position representation\n",
    "        return out\n",
    "\n",
    "    def predict(self, seq, candidates):\n",
    "        seq_repr = self.forward(seq)  # [B, H]\n",
    "        item_repr = self.item_emb(candidates)  # [B, H]\n",
    "        return (seq_repr * item_repr).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4bc632-a8a6-4fd6-9058-7c87a94530a1",
   "metadata": {},
   "source": [
    "## Train & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82759713-3e8e-4eba-af23-f5277c05933f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 4.2063\n",
      "Epoch 2, Loss: 3.4697\n",
      "Epoch 3, Loss: 2.9608\n",
      "Epoch 4, Loss: 2.6624\n",
      "Epoch 5, Loss: 2.3778\n",
      "Epoch 6, Loss: 2.2103\n",
      "Epoch 7, Loss: 1.9235\n",
      "Epoch 8, Loss: 1.7298\n",
      "Epoch 9, Loss: 1.5111\n",
      "Epoch 10, Loss: 1.5157\n",
      "Epoch 11, Loss: 1.3694\n",
      "Epoch 12, Loss: 1.2453\n",
      "Epoch 13, Loss: 1.1094\n",
      "Epoch 14, Loss: 1.0494\n",
      "Epoch 15, Loss: 0.9634\n",
      "Epoch 16, Loss: 0.8558\n",
      "Epoch 17, Loss: 0.8164\n",
      "Epoch 18, Loss: 0.7509\n",
      "Epoch 19, Loss: 0.7098\n",
      "Epoch 20, Loss: 0.6692\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# 4. Training\n",
    "# ====================================================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = SASRecDataset(train_seqs, n_items)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "model = SASRec(n_items, hidden_dim=64, max_len=MAX_SEQ_LEN).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(20):  # small demo run\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for seq, pos, neg in train_loader:\n",
    "        seq, pos, neg = seq.to(device), pos.to(device), neg.to(device)\n",
    "\n",
    "        seq_repr = model(seq)\n",
    "        pos_emb = model.item_emb(pos)\n",
    "        neg_emb = model.item_emb(neg)\n",
    "\n",
    "        pos_score = (seq_repr * pos_emb).sum(dim=-1)\n",
    "        neg_score = (seq_repr * neg_emb).sum(dim=-1)\n",
    "\n",
    "        loss = -torch.mean(torch.log(torch.sigmoid(pos_score - neg_score)))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cedbe387-fb25-49ad-a217-96315897e2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build reverse maps\n",
    "id2item = {v: k for k, v in item2id.items()}\n",
    "\n",
    "# Load movies.dat for movie names\n",
    "movies = pd.read_csv(\n",
    "    \"ml-1m/movies.dat\",\n",
    "    sep=\"::\",\n",
    "    engine=\"python\",\n",
    "    names=[\"item\", \"title\", \"genres\"], encoding='latin-1'\n",
    ")\n",
    "\n",
    "# Map movie IDs to our new indexing system\n",
    "movies[\"item\"] = movies[\"item\"].map(item2id)\n",
    "itemid2name = dict(zip(movies[\"item\"], movies[\"title\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "293955d1-4ab8-431b-b972-2391cdff28c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True item: 2619 -> American Buffalo (1996)\n",
      "Candidates: ['American Buffalo (1996)', 'Life Less Ordinary, A (1997)', 'Shaggy D.A., The (1976)', 'Serpico (1973)', 'Steel (1997)'] ...\n",
      "True item rank: 66 out of 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ALiZaTo\\anaconda3\\envs\\torch-gpu\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:502: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\NestedTensorImpl.cpp:180.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# 5. Prediction Example\n",
    "# ====================================================\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Pick a random user\n",
    "    # u = list(test_seqs.keys())[0]\n",
    "    u = random.choice(list(test_seqs.keys()))\n",
    "    seq = train_seqs[u][-MAX_SEQ_LEN:]\n",
    "    seq_padded = [0]*(MAX_SEQ_LEN - len(seq)) + seq\n",
    "    seq_tensor = torch.tensor([seq_padded], dtype=torch.long).to(device)\n",
    "\n",
    "    # Candidate set: true item + 99 random negatives\n",
    "    true_item = test_seqs[u][0]\n",
    "    candidates = [true_item] + random.sample(range(1, n_items+1), 99)\n",
    "    candidates_tensor = torch.tensor(candidates, dtype=torch.long).to(device)\n",
    "\n",
    "    # Predict scores\n",
    "    scores = model.predict(seq_tensor.repeat(len(candidates), 1),\n",
    "                           candidates_tensor).cpu().numpy().flatten()\n",
    "\n",
    "    ranked = np.argsort(-scores)\n",
    "    rank_of_true = list(ranked).index(0) + 1\n",
    "\n",
    "    # Get names\n",
    "    true_item_name = itemid2name[true_item]\n",
    "    candidates_names = [itemid2name[c] for c in candidates]\n",
    "    \n",
    "    print(\"True item:\", true_item, \"->\", true_item_name)\n",
    "    print(\"Candidates:\", candidates_names[:5], \"...\")  # show a few\n",
    "    print(\"True item rank:\", rank_of_true, \"out of\", len(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41112604-fbb5-4877-a10e-15ebc808887e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# 6. Model Evaluation\n",
    "# ====================================================\n",
    "def evaluate_model(model, train_seqs, test_seqs, n_items, itemid2name, K=10, num_neg=100):\n",
    "    \"\"\"\n",
    "    Evaluate SASRec with Hit@K and NDCG@K.\n",
    "\n",
    "    Args:\n",
    "        model: trained SASRec model\n",
    "        train_seqs: dict {user: [train items]}\n",
    "        test_seqs: dict {user: [test item]}\n",
    "        n_items: total number of items\n",
    "        itemid2name: dict mapping item_id -> movie name\n",
    "        K: cutoff for metrics\n",
    "        num_neg: number of negative samples per test\n",
    "\n",
    "    Returns:\n",
    "        (hit_rate, ndcg)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    hits, ndcgs = [], []\n",
    "    example_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for u in test_seqs:\n",
    "            if len(test_seqs[u]) == 0:\n",
    "                continue\n",
    "\n",
    "            true_item = test_seqs[u][0]\n",
    "            seq = train_seqs[u][-MAX_SEQ_LEN:]\n",
    "            seq_padded = [0]*(MAX_SEQ_LEN - len(seq)) + seq\n",
    "            seq_tensor = torch.tensor([seq_padded], dtype=torch.long).to(device)\n",
    "\n",
    "            # Candidate set: true item + random negatives\n",
    "            candidates = [true_item] + random.sample(range(1, n_items+1), num_neg)\n",
    "            candidates_tensor = torch.tensor(candidates, dtype=torch.long).to(device)\n",
    "\n",
    "            scores = model.predict(seq_tensor.repeat(len(candidates), 1),\n",
    "                                   candidates_tensor).cpu().numpy().flatten()\n",
    "\n",
    "            ranked = np.argsort(-scores)\n",
    "            rank_of_true = list(ranked).index(0)  # 0 = index of true item in candidates\n",
    "\n",
    "            # Metrics\n",
    "            if rank_of_true < K:\n",
    "                hits.append(1)\n",
    "                ndcgs.append(1 / np.log2(rank_of_true + 2))  # rank starts from 0\n",
    "            else:\n",
    "                hits.append(0)\n",
    "                ndcgs.append(0)\n",
    "\n",
    "            # Save some qualitative examples\n",
    "            if len(example_outputs) < 5:  # just show 5 users\n",
    "                topk_idx = ranked[:K]\n",
    "                topk_items = [candidates[i] for i in topk_idx]\n",
    "                topk_names = [itemid2name[it] for it in topk_items]\n",
    "\n",
    "                example_outputs.append({\n",
    "                    \"true_item\": itemid2name[true_item],\n",
    "                    \"topk\": topk_names\n",
    "                })\n",
    "\n",
    "    hit_rate = np.mean(hits)\n",
    "    ndcg = np.mean(ndcgs)\n",
    "\n",
    "    print(f\"Hit@{K}: {hit_rate:.4f}, NDCG@{K}: {ndcg:.4f}\\n\")\n",
    "\n",
    "    print(\"Sample recommendations:\")\n",
    "    for ex in example_outputs:\n",
    "        print(\"True item:\", ex[\"true_item\"])\n",
    "        print(\"Top-K predictions:\", ex[\"topk\"])\n",
    "        print(\"---\")\n",
    "\n",
    "    return hit_rate, ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b79d78c-21cc-433c-8003-583e9eb98b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit@10: 0.2471, NDCG@10: 0.1168\n",
      "\n",
      "Sample recommendations:\n",
      "True item: Pocahontas (1995)\n",
      "Top-K predictions: ['In Search of the Castaways (1962)', 'Soylent Green (1973)', 'Double Indemnity (1944)', 'American President, The (1995)', 'Dead Again (1991)', 'Above the Rim (1994)', 'Thing, The (1982)', 'Only Angels Have Wings (1939)', 'Farewell My Concubine (1993)', 'Topaz (1969)']\n",
      "---\n",
      "True item: Lost World: Jurassic Park, The (1997)\n",
      "Top-K predictions: ['Bowfinger (1999)', 'Fight Club (1999)', 'Father of the Bride Part II (1995)', 'Austin Powers: The Spy Who Shagged Me (1999)', 'Married to the Mob (1988)', 'Stigmata (1999)', 'Lost World: Jurassic Park, The (1997)', 'Replacement Killers, The (1998)', 'Long Goodbye, The (1973)', 'Dog of Flanders, A (1999)']\n",
      "---\n",
      "True item: Little Mermaid, The (1989)\n",
      "Top-K predictions: ['Little Mermaid, The (1989)', 'Rocky (1976)', 'Inventing the Abbotts (1997)', 'Cell, The (2000)', 'Rear Window (1954)', 'Alvarez Kelly (1966)', 'Bandit Queen (1994)', 'Wisdom (1986)', 'Quiz Show (1994)', 'Butterfly Kiss (1995)']\n",
      "---\n",
      "True item: Rocky (1976)\n",
      "Top-K predictions: ['Twelve Monkeys (1995)', 'Original Kings of Comedy, The (2000)', 'Rocky (1976)', 'Big Sleep, The (1946)', 'Small Soldiers (1998)', 'American Pie (1999)', 'Scream (1996)', 'Henry Fool (1997)', 'Dumb & Dumber (1994)', 'Sonic Outlaws (1995)']\n",
      "---\n",
      "True item: Billy's Hollywood Screen Kiss (1997)\n",
      "Top-K predictions: ['Stand by Me (1986)', 'Addiction, The (1995)', 'Freejack (1992)', 'Silence of the Lambs, The (1991)', 'Philadelphia Story, The (1940)', 'Walk in the Clouds, A (1995)', 'Death Wish V: The Face of Death (1994)', 'Transformers: The Movie, The (1986)', 'Roommates (1995)', 'Mister Roberts (1955)']\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "hit, ndcg = evaluate_model(model, train_seqs, test_seqs, n_items, itemid2name, K=10, num_neg=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-GPU-2.5.1",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
