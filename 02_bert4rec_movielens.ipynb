{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41470c2e-7b3e-41e2-bba4-13f6dc188a8c",
   "metadata": {},
   "source": [
    "# BERT4Rec on MovieLens\n",
    "This notebook demonstrates the use of the sequenial recommendation algorithm, **BERT4Rec**, to predict the next movie for a particular user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57895a29-dbf7-4dad-bbe9-9d241713d5c9",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27a44b6d-864d-4f36-bd5b-8fa546bb811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0081fb84-ee23-4764-8ecc-63b65d7f6915",
   "metadata": {},
   "source": [
    "## Load and prepare data\n",
    "***NOTE***  \n",
    "It is assumed that that MovieLens-1M dataset has already been downloaded and placed next to this notebook in a folder named `ml-1m`.\n",
    "\n",
    "This is what'll happen below:\n",
    "- After loading the data, sort each user's sequence of movie ratings chronologically.\n",
    "- If a user's sequence is less than 2 movie ratings long, ignore. Otherwise, use all but the last rating for training, and use the last rating for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "387cc188-1da1-4ec7-af30-717f86846b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1) Load and preprocess MovieLens\n",
    "# =========================\n",
    "MAX_SEQ_LEN = 50\n",
    "\n",
    "# Load ratings\n",
    "ratings = pd.read_csv(\n",
    "    \"ml-1m/ratings.dat\",\n",
    "    sep=\"::\",\n",
    "    engine=\"python\",\n",
    "    names=[\"user\", \"item\", \"rating\", \"timestamp\"]\n",
    ")\n",
    "\n",
    "# Map users and items to ids\n",
    "user2id = {u: i+1 for i, u in enumerate(ratings[\"user\"].unique())}\n",
    "item2id = {m: i+1 for i, m in enumerate(ratings[\"item\"].unique())}\n",
    "id2item = {v: k for k, v in item2id.items()}\n",
    "\n",
    "ratings[\"user\"] = ratings[\"user\"].map(user2id)\n",
    "ratings[\"item\"] = ratings[\"item\"].map(item2id)\n",
    "\n",
    "n_users = len(user2id)\n",
    "n_items = len(item2id)\n",
    "\n",
    "# Load movies for item names\n",
    "movies = pd.read_csv(\n",
    "    \"ml-1m/movies.dat\",\n",
    "    sep=\"::\",\n",
    "    engine=\"python\",\n",
    "    names=[\"item\", \"title\", \"genres\"], encoding='latin-1'\n",
    ")\n",
    "movies[\"item\"] = movies[\"item\"].map(item2id)\n",
    "itemid2name = dict(zip(movies[\"item\"], movies[\"title\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26f6d293-097b-4f15-9932-67c6b2a858f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 2) Build user sequences\n",
    "# =========================\n",
    "user_seqs = defaultdict(list)\n",
    "for u, i, t in ratings[[\"user\", \"item\", \"timestamp\"]].itertuples(index=False):\n",
    "    user_seqs[u].append((t, i))\n",
    "\n",
    "for u in user_seqs:\n",
    "    user_seqs[u] = [x for _, x in sorted(user_seqs[u])]\n",
    "\n",
    "train_seqs, test_seqs = {}, {}\n",
    "for u, seq in user_seqs.items():\n",
    "    if len(seq) < 2:\n",
    "        continue\n",
    "    train_seqs[u] = seq[:-1]\n",
    "    test_seqs[u] = [seq[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7eb0b94-9e9e-4d77-8de2-d10e10f1bc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3) Dataset for BERT4Rec (masked modeling)\n",
    "# =========================\n",
    "MASK_ID = n_items + 1  # special mask token\n",
    "\n",
    "class BERT4RecDataset(Dataset):\n",
    "    def __init__(self, user_seqs, max_len=MAX_SEQ_LEN, mask_prob=0.15):\n",
    "        self.user_seqs = user_seqs\n",
    "        self.users = list(user_seqs.keys())\n",
    "        self.max_len = max_len\n",
    "        self.mask_prob = mask_prob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.users)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        u = self.users[idx]\n",
    "        seq = self.user_seqs[u][-self.max_len:]\n",
    "        seq = [0]*(self.max_len - len(seq)) + seq\n",
    "\n",
    "        seq = torch.tensor(seq, dtype=torch.long)\n",
    "\n",
    "        # Masking\n",
    "        masked_seq = seq.clone()\n",
    "        labels = torch.full_like(seq, -100)  # ignore index\n",
    "        prob = torch.rand(seq.size())\n",
    "        mask = (prob < self.mask_prob) & (seq != 0)\n",
    "        masked_seq[mask] = MASK_ID\n",
    "        labels[mask] = seq[mask]\n",
    "\n",
    "        return masked_seq, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33df9707-74da-47ce-a0ad-91bbf7cca748",
   "metadata": {},
   "source": [
    "## BERT4Rec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f443705-606a-4bf4-92f1-cff0f36248aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 4) BERT4Rec model\n",
    "# =========================\n",
    "class BERT4Rec(nn.Module):\n",
    "    def __init__(self, n_items, hidden_dim=64, max_len=50,\n",
    "                 num_layers=2, num_heads=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.item_emb = nn.Embedding(n_items+2, hidden_dim, padding_idx=0)  # +1 for MASK\n",
    "        self.pos_emb = nn.Embedding(max_len, hidden_dim)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim*4,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.max_len = max_len\n",
    "        self.n_items = n_items\n",
    "\n",
    "    def forward(self, seq):\n",
    "        B, L = seq.shape\n",
    "        pos_ids = torch.arange(L, device=seq.device).unsqueeze(0).expand(B, L)\n",
    "        x = self.item_emb(seq) + self.pos_emb(pos_ids)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "    def predict(self, seq, candidates):\n",
    "        x = self.forward(seq)  # [B, L, H]\n",
    "        last_hidden = x[:, -1, :]  # [B, H]\n",
    "        cand_emb = self.item_emb(candidates)  # [B, C, H]\n",
    "        scores = torch.bmm(cand_emb, last_hidden.unsqueeze(-1)).squeeze(-1)  # [B, C]\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742dd808-9aba-45bf-9f8b-bee1060229b7",
   "metadata": {},
   "source": [
    "## Train & Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b1fae9f-44d6-4e6d-8afa-e94d8107c333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss 22.5920\n",
      "Epoch 2: Loss 16.0852\n",
      "Epoch 3: Loss 13.2235\n",
      "Epoch 4: Loss 11.3859\n",
      "Epoch 5: Loss 10.1377\n",
      "Epoch 6: Loss 9.3468\n",
      "Epoch 7: Loss 8.8279\n",
      "Epoch 8: Loss 8.4794\n",
      "Epoch 9: Loss 8.2370\n",
      "Epoch 10: Loss 8.0493\n",
      "Epoch 11: Loss 7.9462\n",
      "Epoch 12: Loss 7.8436\n",
      "Epoch 13: Loss 7.7658\n",
      "Epoch 14: Loss 7.7183\n",
      "Epoch 15: Loss 7.6657\n",
      "Epoch 16: Loss 7.6474\n",
      "Epoch 17: Loss 7.6018\n",
      "Epoch 18: Loss 7.5964\n",
      "Epoch 19: Loss 7.5768\n",
      "Epoch 20: Loss 7.5515\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 5) Training loop\n",
    "# =========================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERT4Rec(n_items, hidden_dim=64, max_len=MAX_SEQ_LEN).to(device)\n",
    "\n",
    "train_dataset = BERT4RecDataset(train_seqs)\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(20):  # small demo\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for masked_seq, labels in train_loader:\n",
    "        masked_seq, labels = masked_seq.to(device), labels.to(device)\n",
    "        outputs = model(masked_seq)  # [B, L, H]\n",
    "        logits = outputs @ model.item_emb.weight.T  # [B, L, n_items+2]\n",
    "\n",
    "        loss = criterion(logits.view(-1, n_items+2), labels.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}: Loss {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2820b45-b229-46ab-a4cb-f211927d0ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 6) Evaluation (Hit@K, NDCG@K)\n",
    "# =========================\n",
    "def evaluate_model(model, train_seqs, test_seqs, n_items, itemid2name, K=10, num_neg=100):\n",
    "    model.eval()\n",
    "    hits, ndcgs = [], []\n",
    "    example_outputs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for u in test_seqs:\n",
    "            if len(test_seqs[u]) == 0:\n",
    "                continue\n",
    "\n",
    "            true_item = test_seqs[u][0]\n",
    "            seq = train_seqs[u][-MAX_SEQ_LEN:]\n",
    "            seq_padded = [0]*(MAX_SEQ_LEN - len(seq)) + seq\n",
    "            seq_padded[-1] = MASK_ID  # mask last position\n",
    "\n",
    "            seq_tensor = torch.tensor([seq_padded], dtype=torch.long).to(device)\n",
    "\n",
    "            # Candidate set: true + negatives\n",
    "            candidates = [true_item] + random.sample(range(1, n_items+1), num_neg)\n",
    "            candidates_tensor = torch.tensor(candidates, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "            scores = model.predict(seq_tensor, candidates_tensor).cpu().numpy().flatten()\n",
    "            ranked = np.argsort(-scores)\n",
    "            rank_of_true = list(ranked).index(0)\n",
    "\n",
    "            # Metrics\n",
    "            if rank_of_true < K:\n",
    "                hits.append(1)\n",
    "                ndcgs.append(1 / np.log2(rank_of_true + 2))\n",
    "            else:\n",
    "                hits.append(0)\n",
    "                ndcgs.append(0)\n",
    "\n",
    "            if len(example_outputs) < 5:\n",
    "                topk_idx = ranked[:K]\n",
    "                topk_items = [candidates[i] for i in topk_idx]\n",
    "                topk_names = [itemid2name[it] for it in topk_items if it in itemid2name]\n",
    "                example_outputs.append({\n",
    "                    \"true_item\": itemid2name.get(true_item, str(true_item)),\n",
    "                    \"topk\": topk_names\n",
    "                })\n",
    "\n",
    "    hit_rate = np.mean(hits)\n",
    "    ndcg = np.mean(ndcgs)\n",
    "\n",
    "    print(f\"Hit@{K}: {hit_rate:.4f}, NDCG@{K}: {ndcg:.4f}\\n\")\n",
    "    print(\"Sample recommendations:\")\n",
    "    for ex in example_outputs:\n",
    "        print(\"True item:\", ex[\"true_item\"])\n",
    "        print(\"Top-K predictions:\", ex[\"topk\"])\n",
    "        print(\"---\")\n",
    "\n",
    "    return hit_rate, ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1abdc16e-6a33-4cce-8f51-eb9c7a45da9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit@10: 0.3598, NDCG@10: 0.1828\n",
      "\n",
      "Sample recommendations:\n",
      "True item: Pocahontas (1995)\n",
      "Top-K predictions: ['Arachnophobia (1990)', 'Seven (Se7en) (1995)', 'Sleepless in Seattle (1993)', 'Batman Returns (1992)', 'Hoop Dreams (1994)', 'Tao of Steve, The (2000)', 'Outbreak (1995)', \"Pee-wee's Big Adventure (1985)\", 'Basic Instinct (1992)', 'Jackie Brown (1997)']\n",
      "---\n",
      "True item: Armageddon (1998)\n",
      "Top-K predictions: ['Toy Story 2 (1999)', 'When Harry Met Sally... (1989)', 'Armageddon (1998)', 'Shanghai Noon (2000)', 'Seven (Se7en) (1995)', 'Blues Brothers, The (1980)', 'Grease (1978)', 'From Dusk Till Dawn (1996)', 'Star Trek IV: The Voyage Home (1986)', 'Trading Places (1983)']\n",
      "---\n",
      "True item: Little Mermaid, The (1989)\n",
      "Top-K predictions: ['Fight Club (1999)', 'Breakfast Club, The (1985)', 'Hunt for Red October, The (1990)', 'Little Mermaid, The (1989)', 'Get Shorty (1995)', 'Birds, The (1963)', 'Sense and Sensibility (1995)', 'Swingers (1996)', 'Midnight Cowboy (1969)', 'Conspiracy Theory (1997)']\n",
      "---\n",
      "True item: Die Hard (1988)\n",
      "Top-K predictions: ['Terminator 2: Judgment Day (1991)', \"Bug's Life, A (1998)\", 'Blair Witch Project, The (1999)', 'Die Hard (1988)', 'Contender, The (2000)', 'Superman II (1980)', 'Dirty Dancing (1987)', 'Outbreak (1995)', 'Cinema Paradiso (1988)', 'Cruel Intentions (1999)']\n",
      "---\n",
      "True item: Natural Born Killers (1994)\n",
      "Top-K predictions: ['Terminator 2: Judgment Day (1991)', 'Ghostbusters (1984)', \"Schindler's List (1993)\", \"You've Got Mail (1998)\", 'Mad Max 2 (a.k.a. The Road Warrior) (1981)', 'Bowfinger (1999)', 'Superman II (1980)', 'Entrapment (1999)', 'Deliverance (1972)', 'Carrie (1976)']\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "hit, ndcg = evaluate_model(model, train_seqs, test_seqs, n_items, itemid2name, K=10, num_neg=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-GPU-2.5.1",
   "language": "python",
   "name": "pytorch-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
